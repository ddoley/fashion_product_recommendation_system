{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae5945b0",
   "metadata": {},
   "source": [
    "##### Here we not building the CNN model from the scratch,rather we are using pretrained model(already trained on huge dataset).\n",
    "##### So I  have imported the model and configured it according to  requirements\n",
    "##### Here I am using the Resnet50 CNN model(this model is trained on imagenet with more than millions of images)\n",
    "- Resnet50 model can classify images in thousands different objects-i.e it extracts features from the  images\n",
    "##### Here resnet50 model will extract 2400 objects from each image in the fashion project dataset and once we have extracted the featues we have to identify similar images based on the input image features\n",
    "##### Now the features will be feeded to the nearestneighbors alogrithm\n",
    "- here based upon the features of the 44 images distance will be calculated.To identify the similar product image will be considering the nearest image feature based upon the euclidian distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439eae2a",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46216438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50,preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.layers import GlobalMaxPool2D\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import os #to import the images names from image folder\n",
    "from numpy.linalg import norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3236d6f",
   "metadata": {},
   "source": [
    "## Extract filenames from Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87199c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='E:/FTE/Projects/DL/Fashion/images'\n",
    "filenames = []\n",
    "for file in os.listdir(path):\n",
    "    filenames.append(os.path.join(path,file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5531cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d76df2",
   "metadata": {},
   "source": [
    "## Importing ResNet50 Model and Configuration\n",
    "- resnet50 model is trained based on the imagenet dataset\n",
    "- include_top=False   --here we are excluding one layer(top layer) from resnet50\n",
    "- when we give the image into the model we treat it as 3D array of RGB\n",
    "- since we are not including the top layer so we are replacing it with globalmaxpool2D layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e98281",
   "metadata": {},
   "outputs": [],
   "source": [
    "model= ResNet50(weights='imagenet',include_top=False, input_shape=(224,224,3))\n",
    "model.trainable =False  #we are not training the model as we are using pretrained model\n",
    "\n",
    "model=tf.keras.models.Sequential([model,GlobalMaxPool2D()])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94ab192",
   "metadata": {},
   "source": [
    "### *Note\n",
    "- Here we can see  that we used globalmaxpooling2d layer it converted the 4D array to 2D array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261d6fa9",
   "metadata": {},
   "source": [
    "## Extract 2048 features from the images\n",
    "- we will use preprocessing from tensorflow.keras to extract the image\n",
    "- in target size the image will be converted in 224 224 pixels and then convert the image into array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fc58b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img=image.load_img('54656.jpg',target_size=(224,224))\n",
    "img_array= image.img_to_array(img)\n",
    "img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d1bea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2d3e8b",
   "metadata": {},
   "source": [
    "- Our resnet50 model got trained on batches of images so it is expecting batches of images to extract features so here we are giving only one input\n",
    "- So we need to reshape our image array using a feature called dimension array present in numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bc0d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_expand_dim=np.expand_dims(img_array,axis=0)\n",
    "img_expand_dim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b151187e",
   "metadata": {},
   "source": [
    "- Converted from 3D to 4D array which is ready to be fitted into the model\n",
    "- But we cannot directly load this into the model,firstly we have to apply preprocess_input before feeding into the model(Resnet50)\n",
    "- preprocess_input will convert the image data from RGB to BRG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2366b867",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_preprocess= preprocess_input(img_expand_dim)\n",
    "#now we have to fit it to the model and make prediction\n",
    "result=model.predict(img_preprocess).flatten()   ##flatten()--returns array in 1D from 2D\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d454af",
   "metadata": {},
   "source": [
    "- Here the model has extracted the 2048 features from the image\n",
    "- We need to scale the array that we got.Use norm features from numpy to do this\n",
    "- Norm--scale the values between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f6342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_result=result/norm(result)\n",
    "norm_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2692c97f",
   "metadata": {},
   "source": [
    "- We have extracted the features for only 1 image now we have to extract for entire image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4572ad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_images(image_path, model):   \n",
    "    img=image.load_img(image_path,target_size=(224,224)) \n",
    "    img_array= image.img_to_array(img)\n",
    "    img_array\n",
    "    img_expand_dim=np.expand_dims(img_array,axis=0)\n",
    "    img_preprocess= preprocess_input(img_expand_dim)\n",
    "    result=model.predict(img_preprocess).flatten()\n",
    "    norm_result=result/norm(result)\n",
    "    return norm_result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71336ce3",
   "metadata": {},
   "source": [
    "- it will take image_path as input\n",
    "- it will take image_path instead of passsing a singele image as static path\n",
    "- Now we take the input from the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443ef8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_features_from_images(filenames[0],model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4616b7",
   "metadata": {},
   "source": [
    "- we have extracted the features for a single image now extract the features for all images\n",
    "- Store the features in image_features by appending and passing the image path(file) and model\n",
    "- we can use tqdm module to track the progress of the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54019b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0af383",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features=[]\n",
    "for file in tqdm(filenames[0:22000]):\n",
    "    image_features.append(extract_features_from_images(file,model))\n",
    "image_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c310174",
   "metadata": {},
   "source": [
    "### Save the features\n",
    "- using pickle we will save the features\n",
    "- wb-- write binary, rb-- read binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c867c8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(image_features, open('Image_features.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b36075d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(filenames, open('filenames.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4efdb6",
   "metadata": {},
   "source": [
    "- Now after dumping we need to load/open the saved pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa5b364",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image_features=pkl.load(open('Image_features.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed90cd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames=pkl.load(open('filenames.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b329f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image_features = np.array(Image_features, dtype=np.float16)\n",
    "print(Image_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105f458e",
   "metadata": {},
   "source": [
    "- This image features will be containing the 2048 features for each 44441 images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db68445c",
   "metadata": {},
   "source": [
    "### Find the similar images \n",
    "- For this we will be using nearestneighbors algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68907b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors= NearestNeighbors(n_neighbors=6, algorithm='brute', metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc58d9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors.fit(Image_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f28747",
   "metadata": {},
   "source": [
    "- Here n=6 instead of 5 because the first image will be self image and then find the 5 similar image\n",
    "- Now we need a input image to find the next 5 similar image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd738c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image=extract_features_from_images('54656.jpg', model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d188225",
   "metadata": {},
   "source": [
    "- Now we need to find the similar image with nearest distance using the input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c86ffda",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = np.array(input_image, dtype=np.float32)\n",
    "neighbors._fit_X = neighbors._fit_X.astype(np.float32)  # Ensure the model data is float32\n",
    "\n",
    "distance, indices = neighbors.kneighbors([input_image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a739e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b670d8",
   "metadata": {},
   "source": [
    "- These are the 5 images which are related to input image\n",
    "- Print the input images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f85b14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2988bd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('54656.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8628d9c5",
   "metadata": {},
   "source": [
    "- Now find the filenames for the above given indices and then pass it to the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404e2a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2d3e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(filenames, indices):\n",
    "    plt.figure(figsize=(20, 5))  # Create a figure with a specific size\n",
    "\n",
    "    # Loop through the first 5 recommended images\n",
    "    for i in range(5):\n",
    "        img_path = filenames[indices[0][i + 1]]  # Get the path of the recommended image\n",
    "        img = mpimg.imread(img_path)  # Read the image\n",
    "        \n",
    "        # Add a subplot for each image\n",
    "        plt.subplot(1, 5, i + 1)  # 1 row, 5 columns, position i+1\n",
    "        plt.imshow(img)  # Display the image\n",
    "        plt.axis('off')  # Turn off the axes\n",
    "        plt.title(f\"Recommendation {i + 1}\")  # Add a title for each image\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()  # Show all images\n",
    "\n",
    "# Example Usage\n",
    "display_images(filenames, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106ce763",
   "metadata": {},
   "source": [
    "- These are the recommended images for the input image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
